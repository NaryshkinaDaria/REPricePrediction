{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4349d7e9",
   "metadata": {},
   "source": [
    "### Data preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c846935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTransformer(TransformerMixin):\n",
    "    def __init__(self, n_neighbors, columns):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.columns = columns\n",
    "        self.median = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.nn = NearestNeighbors(n_neighbors=self.n_neighbors)\n",
    "        self.nn.fit(X[self.columns], y)\n",
    "        self.price_sqm = (y / X['size']).replace(np.inf, np.nan).values\n",
    "        self.median = np.median(self.price_sqm)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        nn_id = self.nn.kneighbors(X[self.columns])[1]\n",
    "        nn_price = np.nanmean(self.price_sqm[nn_id[:, 1:]], axis=1)\n",
    "        nn_price = np.where(np.isnan(nn_price), self.median, nn_price)\n",
    "        return nn_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61cad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocess(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, feat_to_impute, feat_by_prop_type, upper_threshold, feat_to_drop, departments, \n",
    "                 city_population, n_neighbors):\n",
    "        self.medians = {}\n",
    "        self.medians_by_prop_type = {}\n",
    "        self.feat_to_impute = feat_to_impute\n",
    "        self.feat_by_prop_type = feat_by_prop_type\n",
    "        self.upper_threshold = upper_threshold\n",
    "        self.feat_to_drop = feat_to_drop\n",
    "        self.departments = departments\n",
    "        self.city_population = city_population\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.nn_transformer = NNTransformer(self.n_neighbors, ['approximate_latitude', 'approximate_longitude'])\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self._impute_missing_values(X)\n",
    "        self._get_medians(X)\n",
    "        self.nn_transformer.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = self._impute_missing_values(X)\n",
    "        df = self._impute_median(df)\n",
    "        df = self._feature_engineering(df)\n",
    "        df = self._trimming_outliers(df)\n",
    "        df = self._features_selection(df)\n",
    "        df['nearest_price_sqm'] = self.nn_transformer.transform(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def _impute_missing_values(self, X):\n",
    "\n",
    "        X.loc[X.property_type.isin(['appartement', 'loft', 'chambre', 'duplex', 'gîte', 'péniche', 'atelier']) \n",
    "              & X.land_size.isna(), 'land_size'] = 0\n",
    "\n",
    "        X.loc[X.property_type.isin(['terrain', 'terrain à bâtir', 'parking']) & (X['size'] == X.land_size), 'size'] = 0\n",
    "\n",
    "        rows = X.property_type.isin(['terrain à bâtir', 'terrain', 'parking']) & X.land_size.isna() & ~X['size'].isna()\n",
    "        X.loc[rows, 'land_size'] = X.loc[rows, 'size']\n",
    "        X.loc[rows, 'size'] = 0\n",
    "\n",
    "        X.loc[X.property_type.isin(['terrain', 'terrain à bâtir', 'parking']) & X['size'].isna(), 'size'] = 0\n",
    "\n",
    "        X.loc[X.property_type.isin(['parking', 'terrain', 'terrain à bâtir']) & X.energy_performance_value.isna(), 'energy_performance_value'] = 0\n",
    "        X.loc[X.property_type.isin(['parking', 'terrain', 'terrain à bâtir']) & X.energy_performance_category.isna(), 'energy_performance_category'] = 'Not applicable'\n",
    "        X.loc[X.property_type.isin(['parking', 'terrain', 'terrain à bâtir']) & X.ghg_value.isna(), 'ghg_value'] = 0\n",
    "        X.loc[X.property_type.isin(['parking', 'terrain', 'terrain à bâtir']) & X.ghg_category.isna(), 'ghg_category'] = 'Not applicable'\n",
    "\n",
    "        X['floor'] = X['floor'].fillna(0)\n",
    "\n",
    "        X.loc[X.property_type.isin(['parking', 'terrain', 'terrain à bâtir']) & X.nb_rooms.isna(), 'nb_rooms'] = 0\n",
    "        X.loc[X.property_type.isin(['parking', 'terrain', 'terrain à bâtir']) & X.nb_bedrooms.isna(), 'nb_bedrooms'] = 0\n",
    "        X.loc[X.property_type.isin(['parking', 'terrain', 'terrain à bâtir']) & X.nb_bathrooms.isna(), 'nb_bathrooms'] = 0\n",
    "\n",
    "        X.loc[X['property_type'] == 'chambre', 'nb_bedrooms'] = 1\n",
    "        return X\n",
    "\n",
    "\n",
    "    def _get_medians(self, X):\n",
    "        for col in self.feat_to_impute:\n",
    "            if X[col].dtype == float:\n",
    "                self.medians[col] = X[col].median()\n",
    "            else:\n",
    "                numerical_col = col.replace('category', 'value')\n",
    "                self.medians[col] = X[X[numerical_col] == X[numerical_col].median()][col].iloc[0]\n",
    "        self.medians_by_prop_type = X.groupby('property_type')[feat_by_prop_type].median().to_dict()\n",
    "\n",
    "\n",
    "    def _impute_median(self, X):\n",
    "        for col in self.feat_by_prop_type:\n",
    "            X[col] = X[col].fillna(X['property_type'].map(self.medians_by_prop_type[col]))\n",
    "        for col in self.feat_to_impute:\n",
    "            X[col] = X[col].fillna(self.medians[col])\n",
    "        return X\n",
    "\n",
    "\n",
    "    def _trimming_outliers(self, X):\n",
    "        for col in self.upper_threshold:\n",
    "            X[col] = X[col].apply(lambda x: self.upper_threshold[col] if x > self.upper_threshold[col] else x)\n",
    "        X.loc[X['size'].between(1, 10), 'size'] = 10\n",
    "        X.loc[X.property_type.isin(['appartement', 'loft', 'chambre', 'duplex']) & \n",
    "              (X['size'] > 500) & (X['size'] / (10 * X['nb_rooms']) <= 50), 'size'] = X['size'] / 10\n",
    "        X.loc[X.property_type.isin(['appartement', 'loft', 'chambre', 'duplex']) & \n",
    "              (X['size'] > 500) & (X['size'] / (10 * X['nb_rooms']) > 50), 'size'] = X['size'] / 100\n",
    "        return X\n",
    "\n",
    "\n",
    "    def _feature_engineering(self, X):\n",
    "        X.loc[X['nb_bathrooms'] > 1, 'nb_bathrooms'] = 1\n",
    "        X.loc[X['nb_bedrooms'] > X['nb_rooms'], 'nb_bedrooms'] = X['nb_rooms']\n",
    "        X.loc[X['nb_bathrooms'] > X['nb_rooms'], 'nb_bathrooms'] = X['nb_rooms']\n",
    "\n",
    "        X['department_num'] = X['postal_code'].astype(str).str[:-3].astype(int)\n",
    "        X = pd.merge(X, self.departments, on='department_num', how='left')\n",
    "        X['city_type'] = np.where(X['capital'] == X['city'], 'admin', 'minor')\n",
    "        X = X.drop(['capital', 'department_num'], axis=1)\n",
    "        X.loc[X.city.str.startswith(('lyon', 'marseille')) | X.city == 'bastia', 'city_type'] = 'admin'\n",
    "        X.loc[X.city.str.startswith('paris'), 'city_type'] = 'primary'\n",
    "\n",
    "        X = pd.merge(X, self.city_population, on='city', how='left')\n",
    "        X['city_population'] = X['city_population'].fillna(0)\n",
    "        X['city_population'] = np.where(X['city_population'] > 100000, '>100K', \n",
    "                                        np.where(X['city_population'] < 50000, '<50K', '50-100K'))\n",
    "        X.loc[X.city.str.startswith(('lyon', 'marseille', 'paris')), 'city_population'] = '>100K'\n",
    "        X['bedrooms_over_rooms'] = (X['nb_bedrooms'] / (X['nb_rooms'].replace(0, np.nan))).replace(np.nan, 0)\n",
    "        X['size_over_rooms'] = (X['size'] / (X['nb_rooms'].replace(0, np.nan))).replace(np.nan, 0)\n",
    "        X['size_over_landsize'] = (X['size'] / (X['land_size'].replace(0, np.nan))).replace(np.nan, 0)\n",
    "        return X\n",
    "\n",
    "\n",
    "    def _features_selection(self, X):\n",
    "        X = X.rename(columns={\"nb_bathrooms\": \"has_a_bathroom\", \"nb_parking_places\": \"has_a_parking_place\", \n",
    "                              \"nb_boxes\": \"has_a_box\", \"nb_terraces\": \"has_a_terrace\"})\n",
    "        X = X.drop(self.feat_to_drop, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0616d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mapping_city_type = {'minor': 0, 'admin': 1, 'primary': 2}\n",
    "        self.mapping_population = {'<50K': 0, '50-100K': 1, '>100K': 2}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X['city_type'] = X['city_type'].map(self.mapping_city_type)\n",
    "        X['city_population'] = X['city_population'].map(self.mapping_population)\n",
    "        X[['department_name', 'property_type']] = X[['department_name', 'property_type']].astype(\"category\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_by_prop_type = ['size', 'land_size', 'nb_rooms', 'nb_bedrooms', 'nb_bathrooms']\n",
    "feat_to_impute = ['energy_performance_value', 'energy_performance_category', 'ghg_value', 'ghg_category', 'size', 'land_size', 'nb_rooms', 'nb_bedrooms', 'nb_bathrooms']\n",
    "feat_to_drop = ['exposition', 'city', 'postal_code', 'energy_performance_category', 'ghg_category', 'id_annonce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_threshold = {'size': 2000,\n",
    "                   'land_size': 20000,\n",
    "                   'energy_performance_value': 1000,\n",
    "                   'ghg_value': 400,\n",
    "                   'nb_rooms': 30,\n",
    "                   'floor': 22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0408742",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [\n",
    "    'approximate_latitude',\n",
    "    'approximate_longitude',\n",
    "    'size',\n",
    "    'land_size',\n",
    "    'energy_performance_value',\n",
    "    'ghg_value',\n",
    "    'floor',\n",
    "    'nb_rooms',\n",
    "    'nb_bedrooms',\n",
    "    'nb_photos',\n",
    "    'nearest_price_sqm',\n",
    "    'price_pred'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08aa3e",
   "metadata": {},
   "source": [
    "### Performance evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa48b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 1)\n",
    "    mdae = round(median_absolute_error(y_true, y_pred), 1)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    mape = '{:.2%}'.format(mean_absolute_percentage_error(y_true, y_pred))\n",
    "    mdape = '{:.2%}'.format(((y_true - y_pred) / y_true).abs().median())\n",
    "    r_squared = '{:.2%}'.format(r2_score(y_true, y_pred))\n",
    "    metrics = pd.DataFrame({'mae': mae,\n",
    "                            'mdae': mdae,\n",
    "                            'mape': mape,\n",
    "                            'mdape': mdape,\n",
    "                            'rmse': rmse,\n",
    "                            'r_squared':r_squared},\n",
    "                           index=[0])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf833764",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57255dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnet = efficientnet_b1(weights=EfficientNet_B1_Weights.DEFAULT)\n",
    "\n",
    "img_embedder = nn.Sequential(*list(effnet.children())[:-1])\n",
    "effnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225]),\n",
    "                                    ])\n",
    "    return preprocess(Image.open(image_path)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5338835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd(image_path):\n",
    "    with torch.no_grad():\n",
    "        output = img_embedder(preprocess_image(image_path)).squeeze()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(df, img_dir, saving_path):\n",
    "    df_embed = {}\n",
    "    for id_ann in tqdm(df.id_annonce):\n",
    "        image_embeddings = []\n",
    "        ann_path = os.path.join(img_dir, 'ann_' + str(id_ann))\n",
    "        \n",
    "        for img_name in os.listdir(ann_path):\n",
    "            if not img_name.endswith(('.jpg', '.png')):\n",
    "                continue\n",
    "            embed = embedd(os.path.join(ann_path, img_name))\n",
    "            image_embeddings.append(embed)\n",
    "\n",
    "        image_embeddings = torch.stack(image_embeddings).mean(dim=0)\n",
    "        df_embed[id_ann] = image_embeddings.numpy()\n",
    "\n",
    "    df_embed = pd.DataFrame.from_dict(df_embed, orient='index')\n",
    "    df_embed.rename(columns={'Unnamed: 0': 'id_annonce'}, inplace=True)\n",
    "    df_embed.to_csv(saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac50049",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings(X_train, img_dir, emb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c0246",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd91278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key='best_booster', value=trial.user_attrs['best_booster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2ddee",
   "metadata": {},
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f321dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm(trial):\n",
    "    \n",
    "    model_params = {\n",
    "        'objective': 'regression',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'num_leaves': trial.suggest_categorical('num_leaves', [32, 64, 128, 256, 512]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0, step=0.05),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0, step=0.05),\n",
    "        'subsample_freq': 1,\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100, step=5)\n",
    "    }\n",
    "\n",
    "    prep_params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 5, 50)\n",
    "    }\n",
    "\n",
    "    preprocessor = DataPreprocess(feat_to_impute, feat_by_prop_type, upper_threshold, feat_to_drop, departments, \n",
    "                                  city_population, **prep_params)\n",
    "    col_transform = ColumnTransformer([('numerical_trans', RobustScaler(), numerical)], remainder='passthrough', \n",
    "                                      verbose_feature_names_out=False)\n",
    "    \n",
    "    pipe_prep = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('encode', CategEncoder()),\n",
    "        ('transform', col_transform)]\n",
    "    )\n",
    "    model = LGBMRegressor(**model_params)\n",
    "    pipe = make_pipeline(pipe_prep,\n",
    "                         TransformedTargetRegressor(\n",
    "                             regressor=model,\n",
    "                             func=np.log1p,\n",
    "                             inverse_func=np.expm1\n",
    "                         ),\n",
    "                         verbose=0\n",
    "                        )\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    cv_scores = -cross_val_score(pipe, X_train, y_train, scoring='neg_mean_absolute_percentage_error', cv=10)\n",
    "\n",
    "    trial.set_user_attr(key='best_booster', value=pipe)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dab690",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    lgbm_study = optuna.create_study(study_name='finetuning_lgbm', direction='minimize')\n",
    "    lgbm_study.optimize(objective_lgbm, n_trials=150, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd63be",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgboost(trial):\n",
    "\n",
    "    model_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'enable_categorical': True,\n",
    "        'n_estimators': 2000,\n",
    "        'verbosity': 0,\n",
    "        'random_state': 42,\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0, step=0.05),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0, step=0.05),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20)\n",
    "    }\n",
    "\n",
    "    prep_params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 5, 50)\n",
    "    }\n",
    "\n",
    "    preprocessor = DataPreprocess(feat_to_impute, feat_by_prop_type, upper_threshold, feat_to_drop, departments, city_population, **prep_params)\n",
    "    col_transform = ColumnTransformer([('numerical_trans', RobustScaler(), numerical)], remainder='passthrough', verbose_feature_names_out=False)\n",
    "    \n",
    "    pipe_prep = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('encode', CategEncoder()),\n",
    "        ('transform', col_transform)]\n",
    "    )\n",
    "    model = XGBRegressor(**model_params)\n",
    "    pipe = make_pipeline(pipe_prep,\n",
    "                         TransformedTargetRegressor(\n",
    "                             regressor=model,\n",
    "                             func=np.log1p,\n",
    "                             inverse_func=np.expm1\n",
    "                         ), \n",
    "                         verbose=0\n",
    "                        )\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    cv_scores = -cross_val_score(pipe, X_train, y_train, scoring='neg_mean_absolute_percentage_error', cv=10)\n",
    "\n",
    "    trial.set_user_attr(key='best_booster', value=pipe)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72db62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    xgb_study = optuna.create_study(study_name='finetuning_xgboost', direction = 'minimize')\n",
    "    xgb_study.optimize(objective_xgboost, n_trials=150, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c69ba",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_catboost(trial):\n",
    "    \n",
    "    model_params = {\n",
    "        'cat_features': ['property_type', 'department_name'],\n",
    "        'iterations': 2000,\n",
    "        'verbose': False,\n",
    "        'random_state': 42,\n",
    "        'depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.2, 1.0, step=0.05),\n",
    "        'subsample': trial.suggest_float('subsample', 0.05, 1.0, step=0.05),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100, step=5)\n",
    "    }\n",
    "\n",
    "    prep_params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 5, 50)\n",
    "    }\n",
    "\n",
    "    preprocessor = DataPreprocess(feat_to_impute, feat_by_prop_type, upper_threshold, feat_to_drop, departments, city_population, **prep_params)\n",
    "    col_transform = ColumnTransformer([('numerical_trans', RobustScaler(), numerical)], remainder='passthrough', verbose_feature_names_out=False)\n",
    "    \n",
    "    pipe_prep = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('encode', CategEncoder()),\n",
    "        ('transform', col_transform)]\n",
    "    )\n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    pipe = make_pipeline(pipe_prep,\n",
    "                         TransformedTargetRegressor(\n",
    "                             regressor=model,\n",
    "                             func=np.log1p,\n",
    "                             inverse_func=np.expm1\n",
    "                         ),\n",
    "                         verbose=1\n",
    "                        )\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    cv_scores = -cross_val_score(pipe, X_train, y_train, scoring='neg_mean_absolute_percentage_error', cv=10)\n",
    "\n",
    "    trial.set_user_attr(key='best_booster', value=pipe)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fab406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(action='ignore', message='Mean of empty slice')\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    cat_study = optuna.create_study(study_name='finetuning_catboost', direction = 'minimize')\n",
    "    cat_study.optimize(objective_catboost, n_trials=150, callbacks=[callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
